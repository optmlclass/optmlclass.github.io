<style>
    a:link {text-decoration:none}
    a:visited {text-decoration:none}
    a:active {text-decoration:none}
    a:hover {text-decoration:none; color:#990000}
    body {background-color:#f0f0f0}
    
    .section {margin:10px; max-width:960px}
    
    td {padding:10px}
    
    li {margin:10px}
    </style>
    <head>
    <title>EC500: Optimization for Machine Learning</title>
    </head>
    
    
    <div class="section">
    <h2>EC500: Optimization for Machine Learning</h2>
    Efficient algorithms to train large models on large datasets have been critical to the recent successes 
    in machine learning and deep learning. This course will introduce students to both the theoretical principles
    behind such algorithms as well as practical implementation considerations. 
    Topics include convergence properties of first-order optimization techniques such as 
    stochastic gradient descent, adaptive learning rate schemes, and momentum. 
    Particular focus will be given to the stochastic optimization problems with non-convex loss surfaces 
    typically present in modern deep learning problems. 
    <br>
    <br>
    <a href=https://docs.google.com/document/d/12ShZdAVhnVVizPnEZ4E13xEFu1chWSodBHmbpgTXr9o/>Syllabus with meeting time and zoom link</a> (BU login required)

    <br>
    <br>
    <h2>Topics</h2>
    <ul>
<li>Stochastic Gradient Descent</li>
<li>Momentum-based optimization, and accelerated gradient descent.</li>
<li>Adaptive gradient methods, including AdaGrad and Adam.</li>
<li>Normalized stochastic gradient descent, LARS and LAMB.</li>
<li>Large batch size optimization.</li>
<li>Stochastic preconditioning.</li>
<li>Memory-efficiency techniques.</li>
<li>Learning rate scheduling.</li>
<li>Hyperparameter tuning.</li>
<li>Second-order optimization and hessian-vector products.</li>
<li>Variance reduction.</li>
    </ul>
    <br>
    <br>
    <h2>Prerequisites</h2>
    Ability to program in Python. Some experience with linear algebra, calculus, and probability.
    Example concepts that should be familiar include gradients, eigenvectors, eigenvalues,
    Taylor series, and expectations.
    <br>
    <br>
    <h2>Course Notes</h2>
    <ul>
        <li><a href=notes/mathbasics.pdf>Math Background Reference</a></li>
        <li><a href=notes/notes1_mlbasics.pdf>Notes 1 (ML basics)</a></li>
        <li><a href=notes/notes2_convexgd.pdf>Notes 2 (Convex Gradient Descent)</a></li>
        <li><a href=notes/notes3_gdsmooth.pdf>Notes 3 (Smooth Gradient Descent)</a></li>
        <li><a href=notes/notes4_nonconvexsgd.pdf>Notes 4 (Non-Convex SGD)</a></li>
        <li><a href=notes/notes5_tips.pdf>Notes 5 (Tuning Learning Rates)</a></li>
        <li><a href=notes/notes6_adaptive1.pdf>Notes 6 (Adaptive Learning Rates)</a></li>
        <li><a href=notes/notes7_adaptive2.pdf>Notes 7 (More Adaptive Learning Rates)</a></li>
        <li><a href=notes/notes8_acceleration.pdf>Notes 8 (Acceleration)</a></li>
        <li><a href=notes/notes8p5_lstar.pdf>Notes 8.5 (Smoothness+Adaptivity)</a></li>
        <li><a href=notes/notes9_adagrad.pdf>Notes 9 (Adagrad)</a></li>
        <li><a href=notes/notes10_distributed.pdf>Notes 10 (Distributed Methods)</a></li>
        <li><a href=notes/notes11_momentum.pdf>Notes 11 (Momentum)</a></li>
        <li><a href=notes/notes12_regularization.pdf>Notes 12 (Regularization)</a></li>
        <li><a href=notes/notes13_almostconv.pdf>Notes 13 (Almost Convexity)</a></li>
        <li><a href=notes/notes14_vr.pdf>Notes 14 (Variance Reduction)</a></li>
        <li><a href=notes/notes15_norm.pdf>Notes 15 (Non-Convex Variance Reduction)</a></li>
        <li><a href=notes/notes16_cubic.pdf>Notes 16 (Cubic Regularization)</a></li>
    </ul>
    <br>
    </div>